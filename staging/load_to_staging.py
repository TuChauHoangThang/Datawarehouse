"""
Ng∆∞·ªùi s·ªë 2 - Data Prep Lead (Staging)
Ch·∫°y l·ªánh: python staging\\load_to_staging.py
Nhi·ªám v·ª•:
1. Ki·ªÉm tra rawData, h·ª£p nh·∫•t t·∫•t c·∫£ *_raw.csv
2. Chu·∫©n h√≥a schema, th√™m metadata/source_file
3. ƒê·∫©y snapshot m·ªõi nh·∫•t v√†o staging_dw.stg_stream_snapshot (REPLACE)
"""

import os
import pandas as pd
import yaml
import sqlalchemy

# ===============================
# ‚öôÔ∏è 1. ƒê·ªåC FILE C·∫§U H√åNH YAML
# ===============================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # => D:\DW\staging
CONFIG_PATH = os.path.join(BASE_DIR, "config_dw.yaml")

if not os.path.exists(CONFIG_PATH):
    raise FileNotFoundError(f" Kh√¥ng t√¨m th·∫•y file c·∫•u h√¨nh t·∫°i: {CONFIG_PATH}")

# B∆∞·ªõc 2.1 (Ng∆∞·ªùi 2) - ƒê·ªçc c·∫•u h√¨nh YAML ƒë·ªÉ l·∫•y th√¥ng tin database/ngu·ªìn
with open(CONFIG_PATH, "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

# ===============================
# üß© 2. T·∫†O CHU·ªñI K·∫æT N·ªêI POSTGRESQL
# ===============================
# B∆∞·ªõc 2.2 (Ng∆∞·ªùi 2) - Chu·∫©n b·ªã chu·ªói k·∫øt n·ªëi PostgreSQL staging_dw
staging_info = config.get("staging_db")
if not staging_info:
    raise KeyError(" Kh√¥ng t√¨m th·∫•y kh√≥a 'staging_db' trong file c·∫•u h√¨nh YAML.")

connection_url = (
    f"postgresql+psycopg2://{staging_info['user']}:{staging_info['password']}"
    f"@{staging_info['host']}:{staging_info['port']}/{staging_info['dbname']}"
)

engine = sqlalchemy.create_engine(connection_url)

# ===============================
# üì¶ 3. T√åM V√Ä ƒê·ªåC TO√ÄN B·ªò FILE RAW
# ===============================
# B∆∞·ªõc 2.3 (Ng∆∞·ªùi 2) - Qu√©t th∆∞ m·ª•c rawData t√¨m c√°c file *_raw.csv
raw_data_dir = os.path.join(BASE_DIR, "rawData")
if not os.path.exists(raw_data_dir):
    raise FileNotFoundError(f" Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c rawData t·∫°i: {raw_data_dir}")

csv_files = [f for f in os.listdir(raw_data_dir) if f.endswith("_raw.csv")]
if not csv_files:
    raise FileNotFoundError(f" Kh√¥ng c√≥ file *_raw.csv n√†o trong {raw_data_dir}")

csv_files.sort(key=lambda f: os.path.getctime(os.path.join(raw_data_dir, f)))

dataframes = []
total_records = 0
# B∆∞·ªõc 2.4 (Ng∆∞·ªùi 2) - ƒê·ªçc t·ª´ng file raw v√† chu·∫©n h√≥a schema
print(" ƒêang ƒë·ªçc c√°c file d·ªØ li·ªáu:")
for csv_name in csv_files:
    path = os.path.join(raw_data_dir, csv_name)
    print(f"  ‚Ä¢ {path}")
    df_part = pd.read_csv(path)
    df_part.columns = [c.strip().lower().replace(" ", "_") for c in df_part.columns]

    if "platform" not in df_part.columns:
        if "youtube" in csv_name.lower():
            df_part["platform"] = "YouTube"
        elif "twitch" in csv_name.lower():
            df_part["platform"] = "Twitch"
        else:
            df_part["platform"] = ""

    df_part["source_file"] = csv_name
    dataframes.append(df_part)
    total_records += len(df_part)

if not dataframes:
    raise ValueError(" Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c d·ªØ li·ªáu t·ª´ b·∫•t k·ª≥ file raw n√†o.")

df = pd.concat(dataframes, ignore_index=True).drop_duplicates()
print(f" ƒê√£ ƒë·ªçc {len(df)} d√≤ng d·ªØ li·ªáu t·ª´ {len(dataframes)} file (t·ªïng b·∫£n ghi g·ªëc: {total_records}).")

# ===============================
#  4. GHI D·ªÆ LI·ªÜU V√ÄO POSTGRES (STAGING)
# ===============================
table_name = "stg_stream_snapshot"

# B∆∞·ªõc 2.5 (Ng∆∞·ªùi 2) - Ghi snapshot h·ª£p nh·∫•t v√†o b·∫£ng stg_stream_snapshot (REPLACE)
df.to_sql(table_name, engine, if_exists="replace", index=False)
print(f" ƒê√£ n·∫°p {len(df)} d√≤ng v√†o b·∫£ng '{table_name}' trong database staging_dw")
print(" Qu√° tr√¨nh n·∫°p d·ªØ li·ªáu v√†o STAGING ho√†n t·∫•t!")
